## Empirical Methodology {#model}
### Overview

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Load libraries
library('kableExtra')
library(dplyr)
library(knitr)
options(kableExtra.latex.load_packages = FALSE)
options(knitr.table.format = "pandoc")
options(knitr.kable.NA = '')
```

The total credit series is nonstationary. In order to extract a useful stationary credit signal to use as an early warning indicator for future financial crises, I first decompose the series into a nonstationary trend and a stationary cycle using various popularly studied filters in subsection \@ref(decomp). 

The idea behind using the credit gap as an early warning indicator is that a prolonged period of excessive credit growth could be linked to future financial crises. Therefore, I can use logistic regression to identify pre-crisis periods with unsustainable high credit gap values over a certain threshold.

In subsection \@ref(logistic-regression), I discuss strategies for identifying the dependent variable (pre-crisis periods) and the logistic regression empirical method. In subsection \@ref(metrics) I then discuss the metrics to determine the performance of a credit gap as an early warning indicator EWI model such as BIC, AIC values, Area Under Curve (AUC) of receiver operating characteristic, how I determine a policy loss function, optimize credit gap threshold and estimate the minimized Type I and Type II error rates. I will then propose a novel metric - partial standardized AUC (psAUC) that, when used in conjunction with other metrics, will provide more desirable properties for policy implication and overcome some of the criticism that the traditional AUC metric receives.

This paper aims to overcome model uncertainty when using credit gap filters as an early warning indicator for future financial crises. Since each decomposition filter of the total credit series provides a stationary series that performs qualitatively differently from other filters as an EWI, I propose selecting the best gap candidates and performing model averaging to achieve model stability and improved out-of-sample prediction as proposed in [@raftery_bayesian_1995].

In subsection \@ref(modelselection), I will use the metrics discussed in subsection \@ref(logistic-regression) to select credit gap filters that perform well individually and in combination with other credit gaps. Then in subsection \@ref(model-average), I discuss the theory behind the Bayesian model average method, my model customization to fit the policy implication need, and the new metric (psAUC). 

Lastly, in subsection \@ref(weighted-gap-creation), for ease of policy implication, I propose the creation of a crisis-weighted credit gap that would inherit the information and features from the averaged model of 30 other credit gaps, and would perform as well as the averaged model prediction does.


<!--[Briefly discuss AUC section here]\
[Variable selection here]\
[Model Averaging here]\
[Weighted credit gap creation here]\-->

### Credit Gap Decompositions {#decomp}

\begin{align}
	100*\frac{Credit}{GDP} &= y_it = \tau_{itj} + c_{itj}
\end{align}


I started my variable selection process by creating 90 candidate one-sided credit gap measurements based on the literature.^[List of filters created is can be viewed in Appendix \@ref(filterslist)] The nonstationary Credit to GDP ratio series is decomposed into a nonstationary trend $\tau_{yit}$ and a stationary cycle credit gap $c_{itj}$ components. Once a country has more than 15 years of credit measurement available, I start storing its one-sided credit gap values onward. My decomposition filter methods include one-sided Hodrick-Prescott, Hamilton (panel and non-panel setting), Moving Average, Structural Time-series model, Beveridge-Nelson, linear, quadratic, and polynomial decompositions. All are decomposed with full sample, rolling 15 years and 20 years rolling window features when possible.

Apart from the filters that I already introduced in Chapter 3 - "Measuring Credit Gap" (Equations 3.1 - 3.5), the two additional filters I will use in this chapter are Moving Average, Structural Time-Series model filters that @beltran_optimizing_2021 introduced.^[Refer to their equation representations here in Appendix \@ref(ma-stm-eq)]

<!--Todo:
[List formula references for each decomposition method]\
[Full list of credit cycle created in appendix]\
[strength and weakness of each decomposition method if possible]\-->

### Early Warning Indicator - GLM Logistic Regression {#logistic-regression}

I begin testing the fitness of the EWI model using credit gap measurements by implementing binary logistic regression. I identify the dependent variable as the periods of 5-12 quarters ahead of a systemic financial crisis. The underlying classification model assumes that the credit to GDP ratio gap will increase sharply as a response to an excessive credit increase due to expansionary monetary policy or a sharp decline in GDP, which is the ratio's denominator. When the credit gap is above a certain threshold for a prolonged period, it signals that the economy is at risk of a future financial crisis. The periods with excessive credit before financial crises will be labeled "pre-crisis" periods. 

It is also worth pointing out that there is no consensus on setting the periods to label as pre-crisis. The literature labeled pre-crisis periods with various ranges from 1-16 quarters before a crisis. I decided to use a labeling strategy with ideal policy implication properties. It is recommended that macroprudential policy takes more than four quarters to be effective because of policy implementation lag. As a result, a signal telling that the economy will experience a financial crisis in 1-4 quarters has little policy implication. Therefore, I start labeling the pre-crisis periods in the 5th quarter before a systemic crisis. I stop labeling in the 12th quarter before a crisis because implementing counter-cyclical policy too early will be ineffective in lowering the risk of a future crisis and would only exacerbate economic conditions by unnecessary credit tightening.

Equation \@ref(eq:logit) represents the logistic regression:

\begin{align} (\#eq:logit)
  pre.crisis_{it} \sim c_{itj} 
\end{align}

In which $i$ is the country indicator, and $j$ is the credit gap filter type. The binary dependent variable $pre.crisis_{it}$ takes values of 1 when the economy is between 5-12 quarters before a systemic financial crisis, labeling the periods as "pre-crisis" positive. I discard measurements between 1-4 quarters before a crisis, during, and post-crisis management periods identified in Lo Duca et al. (2017) and Laeven and Valencia (2018). The credit measurements in these periods are discarded to avoid biased estimation as credit gaps behave erratically during a crisis. 

The indicator is set to 0 at other periods. Additionally, pre-crisis periods of imported crises identified in the dataset are also set to 0 since the model aims to capture pre-crisis periods created by a country's excess credit growth, not from exogenous shocks. However, for labeling conformity, I still discard measurements of periods during and post-crisis of these imported crises, as countries experiencing imported crises could also implement reactionary monetary policies.

<!--Todo:
[elaborate on why the strategy of identifying pre-crisis periods is used this way]\
[pros and cons]\
[alternatives]\ (not worth elaborating) (- individual quarters instead of 5-12)
[easy of use: choice]-->

### EWI Performance Metrics {#metrics}

#### Likelihood Values and BIC {-}

The logistic regression model parameters are estimated by maximizing the cross-entropy log-likelihood values. The relationship between the BIC (Bayesian Information Criterion) and likelihood value is expressed in Equation \@ref(eq:BIC-lik). The model selection criterion here is to choose the filter method with a lower BIC value associated with a higher likelihood value and fitness of the general linearized model or a higher degree of freedom by using fewer explanatory variables.

#### Area Under the Curve (AUC) of Receiver Operating Characteristic (ROC) {-}

A receiver operating characteristic (ROC) curve in the EWI literature setting represents True Positive Rate (TPR) and False Positive Rate (FPR) Cartesian coordinates of different credit gap thresholds, which are used as indicators for identifying pre-crisis periods.^[Refer to Figure \@ref(fig:psAUCfig) for illustration] The credit gap thresholds are determined by the logistic regression model predicted response values which range from (0,1) and map increasingly with the credit gap magnitude values.

A logistic model is strictly preferred over another comparable model if all of the coordinates of its ROC curve are closer to the top-left region of the graph, which minimizes FPR and maximizes TPR. However, it is not always the case when I have such a clear-cut difference. Models can have overlapping ROC curves. To simplify the process of model selection in such cases, the area under the curve (AUC) of ROC metric is used.

\begin{align*}
AUC = \int_0^1 TPR d(FPR)
\end{align*}

Each logistic regression with a different gap measurement yields a value of Area Under the Curve (AUC) of receiver operating characteristic. There is an underlying assumption that the higher the AUC value is, the better the overall performance of a credit gap is as an EWI; on average, its ROC curve has coordinates with higher TPR and lower FPR. 

True Positive Rate represents a model's predictive power (P) to identify pre-crisis periods correctly (positive case). $TPR = TP / (TP + FN)$. True Negative Rate represents a model's predictive power to identify normal periods correctly (negative case). $TNR = TN / (TN + FP)$. False Positive Rate represents the Type I error rate that a model misidentifies a calm, normal period (negative case) as a pre-crisis period (positive case). $FPR = 1 - TNR = FP / (FP + TN)$. Lastly, I discuss the False Negative Rate or the Type II error rate at which a model misidentifies a pre-crisis period (positive case) as a normal calm period (negative case). $FNR = 1 - TPR = FN / (TP + FN)$.^[Our notation of Type I and Type II error follows Beltran (2021) which deviated from previous literature.]

Central planners working with classification problems have dual objectives to minimize Type I and Type II errors. To simplify this problem, a policy loss function with the input of the two error types are used: 

\begin{align} (\#eq:policylossold)
L_{\theta}=\alpha TypeI(\theta)+(1-\alpha)TypeII(\theta)
\end{align}

With $\alpha = 0.5$ by default if policymakers are indifferent about the two types of errors. Here, the model selection criteria would be to select minimized policy loss function with the smallest combination of Type I and Type II error rates. 

A binary logistic regression model with a substantially higher AUC value will have smaller sets of Type I and Type II metrics values. Therefore, its optimized credit gap threshold value that minimizes the policy loss function would represent superior Type I and Type II error rates.

<!--[Talk about Policy Loss Function and TPR, FPR]-->
However, because the AUC value is an aggregate value representing information of the whole ROC curve, the area on its lower left corner, where the predictive power of the threshold (TPR) is low, and type II error rates are high, is not relevant for policy implications discussion. Before the Great Financial Crisis of 2008, central planners tended to weigh Type I errors more than Type II errors since the cost of misspecifying a financial crisis was not apparent, and a Type I error meant costing the economy unnecessary credit constraints by implementing reactionary policies. But since 2009, there has been a shift in weighing Type II errors more as the cost of misspecifying a systemic financial crisis became significantly heavier than a Type I error. 

#### Partial Standardised AUC {-}

To overcome the issue of unnecessary information included in the full AUC, an approach to estimate only a partial relevant area under the ROC was proposed in @mcclish_analyzing_1989 and later implemented in the EWI setting by @detken_operationalising_2014. I will also discuss the standardization of the partial AUC metric (psAUC) in Equation \@ref(eq:psAUCeq).

@detken_operationalising_2014 gave their remarks on the usage of partial standardized AUC (AUROC and AUC terms are used interchangeably in the literature):

>"Instead of considering only the full AUROC (e.g. Drehmann and Juselius, 2014), this paper also presents a partial standardized AUROC (psAUROC) that cuts off the area associated with a preference parameter of $\theta<0.5$"

>"While the psAUROC has been used extensively in the area of medical statistics to assess the performance of a classifier only in specific regions of the ROC curve (e.g., McClish, 1989 and Jiang et al., 1996), it is a new approach in the literature evaluating EWMs."

>"The results reported in this paper show that the psAUROC can reveal useful additional information as long as the partial area does not become too restricted."

@detken_operationalising_2014 proposed cutting off the area representing the low values of preference response parameter in a multivariate regression setting. However, this application does not directly translate to a univariate credit gap model. Using a univariate credit gap EWI model, [@beltran_optimizing_2021] constrained the policy loss function to regions where TPR $\ge 2/3$ or Type II error rate $< 1/3$. They then estimated the policy loss function value at different points on the partial ROC curve by assigning different policy preference parameter values $\alpha$.

To merge these two ideas above and to solve the uncertainty issue of having to estimate policy loss function at different policy preference parameter values to determine model performance, I propose to restrict the consideration of the ROC curve to TPR $\ge 2/3$, then estimate the partial standardize psAUC of the restricted ROC curve region instead. Note in Equation \@ref(eq:pAUC), I estimated the partial area under the curve that represents TPR and TNR (instead of TPR and FNR) and took integral along the TPR axis. 

\begin{align} (\#eq:pAUC)
pAUC = \int_{\frac{2}{3}}^1 TNR \, d(TPR) = \int_{\frac{2}{3}}^1 specificity \, d(sensitivity)
\end{align}

Because I am limiting my analysis only to the portion of the ROC curve that satisfies TPR $\ge 2/3$, the policy loss function \@ref(eq:policylossold) used in previous literature tends to have a corner solution of optimized thresholds where TPR = 2/3. I propose using an alternative policy loss function that priorities points on the ROC closest to the top left regions where both Type I and II errors are the lowest:

\begin{align*}
L_{\theta}= TypeI(\theta)^2 + TypeII(\theta)^2 = (1 - sensitivity)^2 + (1 - specificity)^2
\end{align*}

The last step is to standardize the partial AUC value:

```{r psAUCfig, echo=FALSE, out.width='70%', fig.align="center", fig.cap="Procedure to standardize partial AUC"}
knitr::include_graphics('../../../3rdPaper/metadata/pAUC.png')
```

\begin{align} (\#eq:psAUCeq)
psAUC = \frac{1}{2}\left[ 1+ \frac{pAUC - min}{max - min}\right]
\end{align}

The standardization step helps with the comparison of EWI models' performance. Traditional AUC values range from 0 to 1, with 0.5 being the value for a null hypothesis model or an information-less guess. When only estimated partially, the partial AUC value will not retain the same range making it harder to interpret their meaning. Standardizing the partial AUC values restores the ranges of possible values to 0 and 1.

<!--(Discuss how standardization helps compare the performance of different EWI methods)-->

In section \@ref(empirical-results), I will discuss the relevance and performance of the psAUC as an EWI metric and the other metrics introduced in this subsection. 



### Variable Selection {#modelselection}

Our overall methodology goal is to implement model averaging of top candidate variables. In order to achieve that, I need to select top EWI candidates using the EWI performance criteria discussed in the previous section. I also search for models with combinations of variables that, when combined using positive weights, would have a better model fit than a univariate model would. I aim to select 29 credit gap measurements based on these two criteria.

Firstly, I compared performances of individual credit gaps using partial area under the curve (psAUC) values. Table \@ref(tab:varselect) ranked decomposition filters by psAUC. 


```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Load libraries
library('kableExtra')
library(dplyr)
library(knitr)
options(kableExtra.latex.load_packages = FALSE)
options(knitr.table.format = "pandoc")
options(knitr.kable.NA = '')
```

```{r varselect, echo=FALSE, warning=FALSE, message=FALSE}
options(kableExtra.latex.load_packages = FALSE)
options(knitr.table.format = "pandoc")
library('kableExtra')
library(dplyr)
library(knitr)
options(knitr.kable.NA = '')

filepath='../../../3rdPaper/Data/Output/Modelselection_512.csv'
df<-read.csv(filepath, sep = ",", header=TRUE)

#rownames(df) <- df[,1]
name1<- df[,1]
name1<- gsub("_", ".", name1)
name1[which(name1=="c.hp400k")]<-"BIS Basel gap"
df[,1]<-name1

#df<-df[,-1]
df<-df[-c(32:nrow(df)),-c(10:ncol(df))]
#df<-df[-2,]

#colnames(df) <- c("Median", "10pct", "90pct", "Median", "10pct", "90pct", "Median", "10pct", "90pct")

#options(knitr.kable.NA = '')

#df = df %>% mutate_if(is.numeric, format, digits=4)

#kbl(data.frame(x=rnorm(10), y=rnorm(10), x= rnorm(10)), digits = c(1, 4, 4))

kbl(df, "latex", booktabs = T, digits = c(4, 4, 4, 4, 4, 4, 4, 4), escape=FALSE, linesep=c("","", "", "", "\\addlinespace"), caption ='Top 30 credit gap measurements ranked by psAUC',
       row.names = FALSE) %>%
  kable_styling(latex_options=c("striped","scale_down")) %>%
  kable_paper(c("striped")) %>%   
  #kable_styling(latex_options=c("striped","scale_down", "HOLD_position")) %>%
  #add_header_above(c("Parameters" = 1, "VAR2" = 3, "VAR2 1-cross lag" = 3, "VAR2 2-cross lags" = 3)) %>%
  #footnote(general="UK Bayesian regression results") %>%
  column_spec(5, bold = TRUE) #%>% #c(0,0,1,0,0,0,1,0,0,0,0,0,0,0,0))
  #gsub(".(begin|end){table.*}", "", ., perl = TRUE)%>%
  #gsub(".centering", "", ., perl = TRUE)
```

<!--[Discuss data shown on table]\
[- estimated and compute values BIC, AIC, AUC, psAUC]\
[- BIC, AIC : overall fitness of linearized binary logistic regression model]\
[- AUC: Area Under Curve of receiver operating characteristic (ROC) curve]\

[- we then detected the optimized threshold of credit gap that minimize the policy loss function]\
[- Policy loss function: details here]\
- [- Deviated from previous loss function in the literature (closest to top left)]\
- [- Loss function: if not corner solution at TPR = 2/3, discussion would be trivial]\-->


Secondly, I test for performances of multivariate models with combinations of different credit gaps.

\begin{align*}
Model_k :  pre.crisis_{ti} \sim \sum\nolimits_j \beta_j * credit.gap_{tij}
\end{align*}

I implement the gaps combination model selection using Markov Chain Monte Carlo Model Comparison ($MC^3$) method developed by @madigan_bayesian_1995. The method assigns a posterior probability for different credit gaps being selected in the most likely models/combinations with the lowest BIC values. With 90 variables, there are $2^{90} = 10^{26}$ possible subsets of combinations to choose from.  @babecky_banking_2014 used this $MC^3$ method to identify potential variables in multivariate EWS models. For more efficient search implementation, each variable is given increasing prior weights depending on their univariate EWI performance. I then estimated the posterior probability of each variable included in the most likely models using 4,000,000 MCMC iterations. The results are reported in Table \@ref(tab:varselectMC3).


\tiny
```{r varselectMC3, echo=FALSE, warning=FALSE, message=FALSE}
options(kableExtra.latex.load_packages = FALSE)
options(knitr.table.format = "pandoc")
library('kableExtra')
library(dplyr)
library(knitr)
#library(rstudioapi)
#setwd(dirname(getActiveDocumentContext()$path))
#getwd()

filepath='../../../3rdPaper/Data/Output/MC3.csv'
df<-read.csv(filepath, sep = ",", header=TRUE)

#rownames(df) <- df[,1]
name1<- df[,2]
name1<- gsub("_", ".", name1)
name1[which(name1=="c.hp400k")]<-"BIS Basel gap"
df[,2]<-name1
colnames(df)<-c("Variable","Pr(B!=0)","Variable","Pr(B!=0)")

#df<-df[,-1]
df<-df[-c(27:nrow(df)),]
#df<-df[-2,]

#colnames(df) <- c("Median", "10pct", "90pct", "Median", "10pct", "90pct", "Median", "10pct", "90pct")



#df = df %>% mutate_if(is.numeric, format, digits=4)

#kbl(data.frame(x=rnorm(10), y=rnorm(10), x= rnorm(10)), digits = c(1, 4, 4))

kbl(df, "latex", booktabs = T, digits = c(4, 4,4,4), escape=TRUE, linesep=c("","", "", "", "\\addlinespace"), caption = 'Top 25 credit gap measurements ranked by MC3 probability',
       row.names = FALSE) %>%
  kable_styling(latex_options=c("striped")) %>%
  kable_paper(c("striped")) %>%
  #add_header_above(c("Parameters" = 1, "VAR2" = 3, "VAR2 1-cross lag" = 3, "VAR2 2-cross lags" = 3)) %>%
  footnote(general="Pr(B!=0) is the posterior probability of the variable being selected")
  #column_spec(5, bold = TRUE) %>% #c(0,0,1,0,0,0,1,0,0,0,0,0,0,0,0))
  #gsub(".(begin|end){table.*}", "", ., perl = TRUE)%>%
  #gsub(".centering", "", ., perl = TRUE)
```
\normalsize

Using the two results table above, I selected the candidate gaps not only by their psAUC values but also by their other metrics and types of decomposition methods and features to ensure the robustness of the model averaging method. The 29 selected decomposition filters are reported in Figure \@ref(fig:weightgraph). 29 is also the number of recommended variables to start the Model Averaging method in the following subsection.


### Model Averaging {#model-average}

The Bayesian Model Average method is formalized in @raftery_bayesian_1995 to account for model uncertainty and achieve better prediction results in out-of-sample forecasts. Equation \@ref(eq:posteriorprobweight) represents a Model k's posterior probability or weight in the averaging process:
\begin{align} (\#eq:posteriorprobweight)
  P(M_k|D) = \frac{P(D|M_k)P(M_k)}{\sum\nolimits_{l=1}^K P(D|M_l)P(M_l)} 
  \approx \frac{exp(-\frac{1}{2}BIC_k)}{\sum\nolimits_{l=1}^K exp(-\frac{1}{2}BIC_l)}
\end{align}

Where $P(M_k)$ is model prior probability and can be ignored if all models are assumed to have equal prior weights. $P(D|M_k)$ is the marginal likelihood. And $P(D|M_k) \propto exp(-\frac{1}{2}BIC_k)$.

In which: 
\begin{align} (\#eq:BIC-lik)
BIC_k = 2log (Bayesfactor_{sk}) = \chi^2_{sk} - df_klog(n)
\end{align}

The subscript s indicates the saturated model. $\chi^2_{sk}$ is the deviance of model K from the saturated model.  $\chi^2_{sk} = 2(ll(Ms) - ll(Mk))$ . And $ll(Mk)$ is the log-likelihood of model Mk given data D. A model with better fitness will have a higher log-likelihood value, hence smaller deviance and smaller BIC value. $df_k$ is the degree of freedom of model K, and n is the number of observations.

#### Alternative BIC Measurement {-}

I propose using psAUC in addition to the log-likelihood in the measurement of deviance. Hence, an alternative BIC value can be estimated at:

\begin{align} (\#eq:altBIC)
BIC_{alt,k} &= 2log (Bayesfactor_{alt,sk}) \\
&= 2(1000*(psAUC_s-psAUC_k)) - df_klog(n)
\end{align}

I scaled the psAUC value by 1000 since $0<psAUC<1$. Also, by design, the saturated model has $psAUC_s=1$.

#### Final Real-time Search for Best Models Combinations {-}

All of the following search steps described below are done in quasi-real time. As additional quarterly data are added, I will perform the search steps again to find the best possible combinations of models that fit the updated data.

From the set of 29 candidate filter gaps selected in the previous subsection, I add an intercept parameter to make a list of 30 possible variables from which to choose a subset of combinations. The number of possible subsets of combinations is still significantly high at $2^{30} = 10^9$ possible combinations subset, which will cost us computation time and resources. I still need to limit the number of possible subsets further to consider. @furnival_regressions_2000 proposed an algorithm for computing the residual sums of squares for all possible regressions with minimum computing power requirement. With a "leap and bound" technique, the paper found it possible to find the best subsets without examining all possible subsets. This feature reduced the number of operations required to find the best subsets by several orders of magnitude. 

From the list of reduced subsets using "leap and bound" algorithm, I further filter the number of possible gaps combinations using a method called Occam's razor window described in @raftery_bayesian_1995. The Bayesian Model Average (BMA) R package, which incorporated "leap and bound" algorithm, also has an inbuilt function to filter out less likely models using Occam's razor window method. The package also allowed for a multiple-pass filter through a custom function. 

In this variable selection step using Occam's Razor, I will first select the models with BIC values within an Occam's Razor window of the best-fitted model with the lowest BIC value. Secondly, I repeat the same step above but regarding the alternative BIC value derived from psAUC as in Equation \@ref(eq:altBIC).

Lastly, I filter out models with negative weight combinations for model averaging stability. In order to reach feasible results in these three-pass filter steps, I relax the Occam's razor (OC) value from the default value of 20 to 2000, corresponding to a change in the actual search window ( $2*ln(OC)$ ) that searches for models with 6 times less likelihood value to 15 times.


#### Posterior Distribution of Coefficients of Interest {-}

With all possible candidate models of credit gap combinations selected, I can start implementing the model averaging steps. First, I estimate each model k separately, then store their maximum likelihood estimated parameters $\beta_j(k)$ and their standard deviations. I use the notation $\beta_j(k)$ as the coefficient of credit gap j ($c_j$) in a logistic regression model k against pre-crisis indicator.

I am interested in the averaged estimate of the parameter $\beta_j(K)$ across all models Ks, this is the estimate of the parameter of interest $\beta_j$ in an averaged model. When considering a particular $\beta_1$, the parameter has this probability distribution in a Bayesian averaged model setting:

\begin{align} (\#eq:postprob)
p(\beta_1|D, \beta_1\ne 0) = \sum\nolimits_{A_1} p(\beta_1|D,M_k)p'(M_k|D)
\end{align}

Where $p'(M_k|D)=p(M_k|D)/ pr[\beta_1 \ne 0|D]$. And $pr[\beta_1 \ne 0|D] = \sum\limits_{A_1} p(M_k|D)$. 

$p(M_k|D)$ is the posterior probability discussed in Equation \@ref(eq:posteriorprobweight). While $pr[\beta_1 \ne 0|D]$ is the probability that $\beta_1$ is in the averaged model; and $A_1= \{M_k: k=1,...,K; \beta_1 \ne 0\}$ is the set of models that includes $\beta_1$.


#### Estimation of Coefficients of Interest {-}

From Equation \@ref(eq:postprob), an expected value of the parameter of interest $\beta_1$ could be estimated at:

\begin{align}
\hat{\beta}_1 &= E[\beta_1|D, \beta_1\ne 0] = \sum\limits_{A_1} \hat{\beta}_1(k)p'(M_k|D)
\\
SD^2[\beta_1|D, \beta_1\ne 0] &=[\sum\limits_{A_1}[se_1^2(k)+]+\hat{\beta_1}(k)]p'(M_k|D)
- E[\beta_1|D, \beta_1\ne 0]^2
\end{align}


Where $\hat{\beta}_1(k)$ and $se_1^2(k)$ are respectively the MLE and standard error of $\beta_1$ under the model $M_k$.

So far, I have established averaging estimated parameters using the Bayesian Model Average framework. The response values predicted by the generalized linear model regression can perform better than any individual model in out-of-sample prediction. However, this approach only solves half of the model uncertainty problem. Since I use a multivariate model, deducting an optimized credit gap threshold is still impossible through minimizing the policy loss function. I propose a solution to this issue in the following subsection.

### Weighted Credit Gap Creation {#weighted-gap-creation}

Our motivation to create an averaged weighted gap is to find a solution to the issue of not being able to determine an optimized credit threshold discussed above. Since all credit gap variables measure one set of information: how much in absolute value the total credit series deviated from its long-run trend, I can strategically assign weights to them to create a weighted credit gap.

Our objective is to assign weights to the credit gaps to combine them and create a single credit gap that performs as well as the multivariate Bayesian averaged model in the previous subsection \@ref(model-average). After creating the weighted cycle, I can perform an optimized credit gap threshold analysis, assign a policy loss function value, and compare its EWI performance with other credit gap filters.

Multivariate GLM binary logistic predicted response values: 
\begin{align} (\#eq:glmpredicteq)
\widehat{pre.crisis}_{ti} = \widehat{probability}_{ti} = \frac {1}{1+exp(-(a+\sum\nolimits_j \hat{\beta}_j c_{tij}))}
\end{align}

From the previous subsection \@ref(model-average), in a Bayesian averaged model: $\hat{\beta}_j$ = $E[\beta_j|D, \beta_j\ne 0] = \sum\limits_{A_j} \hat{\beta}_j(k)p'(M_k|D)$. 

Using the information in Equation \@ref(eq:glmpredicteq), I propose creating a single weighted credit gap $\hat{c}_{ti}$ that satisfies:
\begin{align*}
\frac {1}{1+exp(-(a+\hat{\beta} \hat{c}_{ti}))}= \frac {1}{1+exp(-(a+\sum\nolimits_j \hat{\beta}_j c_{tij}))} \\
\end{align*}
Or
\begin{align}
\hat{\beta} \hat{c}_{ti} = \sum\limits_j \hat{\beta}_j c_{tij}
\end{align}

This condition allows us to create a weighted gap $\hat{c}_{ti}$ that will have similar predicted response values and averaged characteristics of the averaged decomposition filters selected. Additionally, to further simplify construction of the weighted gap, I then propose $\hat{\beta} = \sum\nolimits_j \hat{\beta}_j$.

Therefore, 

\begin{align}
\hat{c}_{ti} = \frac{\sum\nolimits_j (\hat{\beta}_j c_{tij})}{\hat{\beta}} = \frac{\sum\nolimits_j (\hat{\beta}_j c_{tij})}{\sum\nolimits_j\hat{\beta}_j} = \sum\nolimits_j w_j c_{tij}
\end{align}

The weight of each candidate credit gap j is $w_j = \frac{\hat{\beta}_j}{\sum\nolimits_j\hat{\beta}_j}$, which is the fraction of the estimate of coefficient $\beta_j$ in the averaged model over the sum of the estimated coefficients of all selected credit gaps.

#### One-sided Crisis-weighted Averaged Credit Gap {-}

I save the weights $w_j$ at every incremental period $t$ of available data to create a one-sided weight vector $w_{tj}$. In the next section, Figure \@ref(fig:weightgraph) illustrates time series of changes in the weights of credit gaps. I estimate the first 15 years of data available for stable model implementation as a full sample regression and assign the weights as constant during that period.

To create one-sided crisis weighted averaged credit gap for each country $i$ ($\hat{c}_{ti}$), I compute:

\begin{align}
\hat{c}_{ti,one-sided} = \sum\nolimits_{j} w_{tj} * c_{tij}
\end{align}

At this point, I have established a framework to create a one-sided crisis weighted averaged credit gap. In the next section, Empirical Results, I will compare the performance as an EWI of this one-sided crisis weighted credit gap (one-sided weighted gap) with other top candidate credit gaps.

